# -*- coding: utf-8 -*-
import os
from dotenv import load_dotenv
import chardet




# 加载环境变量
load_dotenv()
# 从环境变量中读取api_key
api_key = os.getenv('DASHSCOPE_API_KEY')
base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
chat_model = "qwen-plus"
emb_model = "text-embedding-v3"
# 构造client(只需要两个东西)
from openai import OpenAI
client = OpenAI(
    api_key = api_key,
    base_url = base_url
)
# 构造文档
# embedding_text = """
# Multimodal Agent AI systems have many applications. In addition to interactive AI, grounded multimodal models could help drive content generation for bots and AI agents, and assist in productivity applications, helping to re-play, paraphrase, action prediction or synthesize 3D or 2D scenario. Fundamental advances in agent AI help contribute towards these goals and many would benefit from a greater understanding of how to model embodied and empathetic in a simulate reality or a real world. Arguably many of these applications could have positive benefits.
#
# However, this technology could also be used by bad actors. Agent AI systems that generate content can be used to manipulate or deceive people. Therefore, it is very important that this technology is developed in accordance with responsible AI guidelines. For example, explicitly communicating to users that content is generated by an AI system and providing the user with controls in order to customize such a system. It is possible the Agent AI could be used to develop new methods to detect manipulative content - partly because it is rich with hallucination performance of large foundation model - and thus help address another real world problem.
#
# For examples, 1) in health topic, ethical deployment of LLM and VLM agents, especially in sensitive domains like healthcare, is paramount. AI agents trained on biased data could potentially worsen health disparities by providing inaccurate diagnoses for underrepresented groups. Moreover, the handling of sensitive patient data by AI agents raises significant privacy and confidentiality concerns. 2) In the gaming industry, AI agents could transform the role of developers, shifting their focus from scripting non-player characters to refining agent learning processes. Similarly, adaptive robotic systems could redefine manufacturing roles, necessitating new skill sets rather than replacing human workers. Navigating these transitions responsibly is vital to minimize potential socio-economic disruptions.
#
# Furthermore, the agent AI focuses on learning collaboration policy in simulation and there is some risk if directly applying the policy to the real world due to the distribution shift. Robust testing and continual safety monitoring mechanisms should be put in place to minimize risks of unpredictable behaviors in real-world scenarios. Our “VideoAnalytica" dataset is collected from the Internet and considering which is not a fully representative source, so we already go through-ed the ethical review and legal process from both Microsoft and University Washington. Be that as it may, we also need to understand biases that might exist in this corpus. Data distributions can be characterized in many ways. In this workshop, we have captured how the agent level distribution in our dataset is different from other existing datasets. However, there is much more than could be included in a single dataset or workshop. We would argue that there is a need for more approaches or discussion linked to real tasks or topics and that by making these data or system available.
#
# We will dedicate a segment of our project to discussing these ethical issues, exploring potential mitigation strategies, and deploying a responsible multi-modal AI agent. We hope to help more researchers answer these questions together via this paper.
#
# """


embedding_text = """
多模态代理AI系统具有许多应用。除了交互式AI外，基于多模态模型的系统还可以帮助驱动机器人和AI代理的内容生成，并协助生产力应用，帮助重放、释义、预测行动或合成3D或2D场景。代理AI的基础进展有助于实现这些目标，并且许多人将从更好地理解如何在模拟现实或现实世界中建模具身和共情中受益。可以说，这些应用中的许多都可能带来积极的好处。
然而，这项技术也可能被不良行为者利用。生成内容的代理AI系统可能被用来操纵或欺骗人们。因此，非常重要的是，这项技术的开发必须遵循负责任的AI准则。例如，明确告知用户内容是由AI系统生成的，并为用户提供定制此类系统的控制权。代理AI有可能被用来开发新的方法来检测操纵性内容——部分原因是它富含大型基础模型的幻觉表现——从而帮助解决另一个现实世界的问题。
例如，1）在健康领域，尤其是在医疗保健等敏感领域，LLM和VLM代理的伦理部署至关重要。基于偏见数据训练的AI代理可能会通过为代表性不足的群体提供不准确的诊断而加剧健康不平等。此外，AI代理处理敏感患者数据引发了重大的隐私和保密问题。2）在游戏行业，AI代理可能会改变开发者的角色，将他们的注意力从编写非玩家角色脚本转移到优化代理学习过程上。同样，自适应机器人系统可能会重新定义制造业的角色，需要新的技能而不是取代人类工人。负责任地应对这些转变对于尽量减少潜在的社会经济危害至关重要。此外，代理AI专注于在模拟中学习协作策略，如果直接将策略应用于现实世界，由于分布变化，存在一定风险。应建立稳健的测试和持续的安全监控机制，以尽量减少在现实世界场景中出现不可预测行为的风险。我们的“VideoAnalytica”数据集是从互联网上收集的，考虑到它并不是一个完全具有代表性的来源，因此我们已经通过了微软和华盛顿大学的伦理审查和法律程序。
尽管如此，我们还需要理解该数据集中可能存在的偏见。数据分布可以通过多种方式表征。在本研讨会中，我们已经捕捉到我们数据集中代理级别的分布与其他现有数据集的不同之处。然而，单个数据集或研讨会所能涵盖的内容远不止这些。我们认为，需要更多与真实任务或主题相关的方法或讨论，并通过提供这些数据或系统来推动进展。我们将在项目中专门讨论这些伦理问题，探索潜在的缓解策略，并部署一个负责任的多模态AI代理。我们希望通过本文帮助更多研究人员共同回答这些问题。

"""
# # 设置每个文本块大小为150个字符  |切片是左闭右开
# embedding_text = "你好，世界"
chunk_size = 50
chunks = [embedding_text[i:(i+chunk_size)]for i in range(0,len(embedding_text),chunk_size)]
# print(chunks)

# 向量化
from sklearn.preprocessing import normalize
import numpy as np
import faiss
# 初始化一个空列表来存储每个文本块的嵌入向量
text_emb=[]
# 遍历每个文本块
for chunk in chunks:
# 使用openAI API为当前文本块创建嵌入向量
    response = client.embeddings.create(
        model = emb_model,
        input = chunk
    )
    # 将嵌入向量添加到列表中
    text_emb.append(response.data[0].embedding)

# 使用sklearn的normalize函数对嵌入向量进行归一化处理
normalized_embedding = normalize(np.array(text_emb).astype('float32'))
# 获取嵌入向量的维度
shape = len(text_emb[0])
print(shape)
# 创建一个Faiss索引，用于存储和检索嵌入向量
index = faiss.IndexFlatIP(shape)

# 将归一化后的嵌入向量添加到索引中
index.add(normalized_embedding)
# 获取索引中的向量总数
n_vectors =index.ntotal

print(n_vectors)
# 到此将20块文本块转化为20个向量块
#定义一个max_text函数 作用：实现输入向量与各个嵌入向量的距离最近的嵌入向量的匹配，（找到最相似文本）， k是要返回的相似文本块的数量。

def match_text(input_text,index,chunks,k=2):
    """
    在给定的文本块集合中找到与输入文本最相似的前k个文本块。
    参数：
    input_text(str):要匹配的输入文本
    index(faiss.Index):用于搜索的Faiss索引
    chunks(list of str):文本块的列表
    k(int,optional):要返回的最相似文本块的数量。默认值为2
    返回：
    str:格式化的字符串，包含最相似的文本块及其相似度。

    """
    # 确保k不超过文本块的总数
    if k>len(chunks):
        raise ValueError("k值超出文本块数")
    # k = min(k,len(chunks))
    # 使用openAI API为输入文本创建嵌入向量
    response =client.embeddings.create(
        model = emb_model,
        input = input_text
    )
    # 获取输入文本的嵌入向量
    input_embedding = response.data[0].embedding

    # 对输入嵌入向量进行归一化处理
    normalized_input_embedding = normalize(np.array([input_embedding]).astype('float32'))
    # 在索引中搜索与输入向量最相似的k个向量
    distances,indices = index.search(normalized_input_embedding,k)
    # 初始化一个字符串来存储匹配的文本
    match_str = " "

    # 遍历搜索结果
    for i,idx in enumerate(indices[0]) :
    #   1.打印每个匹配文本块的相似度和文本内容
        print(f"similarity:{distances[0][i]:.4f}\t content:{chunks[idx]}")
    #   2.将相似度和文本内容添加到匹配文本字符串中
        match_str +=f"similarity:{distances[0][i]}\t content:{chunks[idx]}"
    # 返回包含匹配文本块及其相似度的字符串
    return match_str
# input_str =input("请输入查找的文本内容: ")
input_str = "What are the applications of Agent AI systems ?"
matched_text = match_text(input_str,index,chunks,k=3)
# 构造prompt环节
prompt = f"""
根据找到的文档{matched_text}生成
{input_str}的答案，尽可能使用文档语句的原文回答，不要复述问题，直接开始回答
"""
# 构建对话引擎  生成对话需用到前面的chat_model
def get_completion_stream(prompt):
    """
    使用openAI的Chat Completions API生成流式的文本回复
    参数：prompt(str):要生成回复的提示文本
    返回：
    None:该函数直接打印生成的回复内容

    """
#     使用OpenAI的Chat Completions API创建一个聊天完成请求
    response  = client.chat.completions.create(
        model = chat_model,
        messages = [{"role":"user","content":prompt},
        ],
        stream = True,
    )
#     如果响应存在
    if response:
    # 遍历响应中的每一块
        for chunk in response:
            # 获取当前块的内容
            chunk_content = chunk.choices[0].delta.content
            # 如果内容存在
            if chunk_content:
            # 打印内容，并刷新输出缓冲区
                print(chunk_content,end='',flush=True)


get_completion_stream(prompt)